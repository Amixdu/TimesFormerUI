{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UIFinal.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1I2RlPymr-e2R_uewd63fDBr30mjSmWYT",
      "authorship_tag": "ABX9TyMKEStNAlt4pIDmubMeEz8b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amixdu/TimesFormerUI/blob/main/UIFinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhhTVOYJs7L7",
        "outputId": "d8d4dc6c-e098-4cdc-fbe9-27c763c11b67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✨🍰✨ Everything looks OK!\n",
            "conda 4.9.2\n"
          ]
        }
      ],
      "source": [
        "# Install Conda\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "!conda --version"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell will download and install all the required packages by the TimesFormer and activate the virtual environment"
      ],
      "metadata": {
        "id": "Eiguotqu5J4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Conda Virtual Environment\n",
        "!conda create -n timesformer python=3.7 -y\n",
        "\n",
        "# Activate Virtual Environment\n",
        "!source activate timesformer\n",
        "\n",
        "# Install Required Packages\n",
        "!pip install torchvision --quiet\n",
        "!pip install 'git+https://github.com/facebookresearch/fvcore' --quiet\n",
        "!pip install simplejson --quiet\n",
        "!pip install einops --quiet\n",
        "!pip install timm --quiet\n",
        "!conda install av -c conda-forge --quiet\n",
        "!pip install psutil --quiet\n",
        "!pip install scikit-learn --quiet\n",
        "!pip install opencv-python --quiet\n",
        "!pip install tensorboard --quiet\n",
        "!pip install matplotlib --quiet\n",
        "!conda install ipykernel --quiet\n",
        "!conda install ipython_genutils --quiet\n",
        "!pip install --upgrade jupyter-client --quiet\n",
        "!pip install flask-ngrok Flask-RESTful flasgger  --quiet\n",
        "\n",
        "# Clone into our github repo (forked from the facebook timesformer repo)\n",
        "def clone():\n",
        "  !git clone https://github.com/Amixdu/TimeSformer.git\n",
        "  %cd TimeSformer\n",
        "  !python setup.py build develop\n",
        "  !mkdir checkpoints\n",
        "  !mkdir input\n",
        "\n",
        "clone()\n",
        "\n",
        "\n",
        "input_folder = \"/content/TimeSformer/input/\"\n",
        "dest = input_folder + \"test.csv\"\n",
        "with open(dest, 'w', encoding='UTF8', newline='') as f:\n",
        "    pass\n",
        "\n",
        "\n",
        "# Moving the checkpoint to TimesFormer directory (for now just connect your drive and move the chekpoint, later can add drop box link or sth)\n",
        "import shutil\n",
        "shutil.copy(\"/content/drive/MyDrive/FIT3162Dataset/Checkpoints/checkpoint_epoch_00015.pyth\", \"/content/TimeSformer/checkpoints\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xlnm86RmtIme",
        "outputId": "ebbf821d-ebd0-4771-9598-d887c268cbb4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\b| \b\bdone\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "  current version: 4.9.2\n",
            "  latest version: 4.12.0\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base conda\n",
            "\n",
            "\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local/envs/timesformer\n",
            "\n",
            "  added / updated specs:\n",
            "    - python=3.7\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    _openmp_mutex-4.5          |            2_gnu          23 KB  conda-forge\n",
            "    ca-certificates-2021.10.8  |       ha878542_0         139 KB  conda-forge\n",
            "    ld_impl_linux-64-2.36.1    |       hea4e1c9_2         667 KB  conda-forge\n",
            "    libffi-3.4.2               |       h7f98852_5          57 KB  conda-forge\n",
            "    libgcc-ng-11.2.0           |      h1d223b6_16         902 KB  conda-forge\n",
            "    libgomp-11.2.0             |      h1d223b6_16         428 KB  conda-forge\n",
            "    libnsl-2.0.0               |       h7f98852_0          31 KB  conda-forge\n",
            "    libstdcxx-ng-11.2.0        |      he4da1e4_16         4.2 MB  conda-forge\n",
            "    libzlib-1.2.11             |    h166bdaf_1014          60 KB  conda-forge\n",
            "    ncurses-6.3                |       h27087fc_1        1002 KB  conda-forge\n",
            "    openssl-3.0.3              |       h166bdaf_0         2.9 MB  conda-forge\n",
            "    pip-22.0.4                 |     pyhd8ed1ab_0         1.5 MB  conda-forge\n",
            "    python-3.7.12              |hf930737_100_cpython        57.3 MB  conda-forge\n",
            "    python_abi-3.7             |          2_cp37m           4 KB  conda-forge\n",
            "    readline-8.1               |       h46c0cb4_0         295 KB  conda-forge\n",
            "    setuptools-62.1.0          |   py37h89c1867_0         1.3 MB  conda-forge\n",
            "    sqlite-3.38.4              |       h4ff8645_0         1.5 MB  conda-forge\n",
            "    tk-8.6.12                  |       h27826a3_0         3.3 MB  conda-forge\n",
            "    wheel-0.37.1               |     pyhd8ed1ab_0          31 KB  conda-forge\n",
            "    zlib-1.2.11                |    h166bdaf_1014          88 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        75.7 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge\n",
            "  _openmp_mutex      conda-forge/linux-64::_openmp_mutex-4.5-2_gnu\n",
            "  ca-certificates    conda-forge/linux-64::ca-certificates-2021.10.8-ha878542_0\n",
            "  ld_impl_linux-64   conda-forge/linux-64::ld_impl_linux-64-2.36.1-hea4e1c9_2\n",
            "  libffi             conda-forge/linux-64::libffi-3.4.2-h7f98852_5\n",
            "  libgcc-ng          conda-forge/linux-64::libgcc-ng-11.2.0-h1d223b6_16\n",
            "  libgomp            conda-forge/linux-64::libgomp-11.2.0-h1d223b6_16\n",
            "  libnsl             conda-forge/linux-64::libnsl-2.0.0-h7f98852_0\n",
            "  libstdcxx-ng       conda-forge/linux-64::libstdcxx-ng-11.2.0-he4da1e4_16\n",
            "  libzlib            conda-forge/linux-64::libzlib-1.2.11-h166bdaf_1014\n",
            "  ncurses            conda-forge/linux-64::ncurses-6.3-h27087fc_1\n",
            "  openssl            conda-forge/linux-64::openssl-3.0.3-h166bdaf_0\n",
            "  pip                conda-forge/noarch::pip-22.0.4-pyhd8ed1ab_0\n",
            "  python             conda-forge/linux-64::python-3.7.12-hf930737_100_cpython\n",
            "  python_abi         conda-forge/linux-64::python_abi-3.7-2_cp37m\n",
            "  readline           conda-forge/linux-64::readline-8.1-h46c0cb4_0\n",
            "  setuptools         conda-forge/linux-64::setuptools-62.1.0-py37h89c1867_0\n",
            "  sqlite             conda-forge/linux-64::sqlite-3.38.4-h4ff8645_0\n",
            "  tk                 conda-forge/linux-64::tk-8.6.12-h27826a3_0\n",
            "  wheel              conda-forge/noarch::wheel-0.37.1-pyhd8ed1ab_0\n",
            "  xz                 conda-forge/linux-64::xz-5.2.5-h516909a_1\n",
            "  zlib               conda-forge/linux-64::zlib-1.2.11-h166bdaf_1014\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "libnsl-2.0.0         | 31 KB     | : 100% 1.0/1 [00:00<00:00, 15.39it/s]\n",
            "wheel-0.37.1         | 31 KB     | : 100% 1.0/1 [00:00<00:00, 24.21it/s]\n",
            "libzlib-1.2.11       | 60 KB     | : 100% 1.0/1 [00:00<00:00, 26.85it/s]\n",
            "setuptools-62.1.0    | 1.3 MB    | : 100% 1.0/1 [00:00<00:00,  3.08it/s]\n",
            "_openmp_mutex-4.5    | 23 KB     | : 100% 1.0/1 [00:00<00:00, 30.13it/s]\n",
            "python_abi-3.7       | 4 KB      | : 100% 1.0/1 [00:00<00:00, 34.92it/s]\n",
            "zlib-1.2.11          | 88 KB     | : 100% 1.0/1 [00:00<00:00, 28.80it/s]\n",
            "openssl-3.0.3        | 2.9 MB    | : 100% 1.0/1 [00:00<00:00,  2.25it/s]\n",
            "libgomp-11.2.0       | 428 KB    | : 100% 1.0/1 [00:00<00:00, 12.50it/s]\n",
            "ncurses-6.3          | 1002 KB   | : 100% 1.0/1 [00:00<00:00,  2.66it/s]\n",
            "libgcc-ng-11.2.0     | 902 KB    | : 100% 1.0/1 [00:00<00:00,  6.59it/s]\n",
            "sqlite-3.38.4        | 1.5 MB    | : 100% 1.0/1 [00:00<00:00,  3.86it/s]\n",
            "libstdcxx-ng-11.2.0  | 4.2 MB    | : 100% 1.0/1 [00:00<00:00,  1.55it/s]\n",
            "python-3.7.12        | 57.3 MB   | : 100% 1.0/1 [00:07<00:00,  7.07s/it]               \n",
            "ld_impl_linux-64-2.3 | 667 KB    | : 100% 1.0/1 [00:00<00:00,  6.65it/s]\n",
            "ca-certificates-2021 | 139 KB    | : 100% 1.0/1 [00:00<00:00, 22.41it/s]\n",
            "readline-8.1         | 295 KB    | : 100% 1.0/1 [00:00<00:00, 12.44it/s]\n",
            "tk-8.6.12            | 3.3 MB    | : 100% 1.0/1 [00:00<00:00,  1.80it/s]\n",
            "pip-22.0.4           | 1.5 MB    | : 100% 1.0/1 [00:00<00:00,  2.95it/s]\n",
            "libffi-3.4.2         | 57 KB     | : 100% 1.0/1 [00:00<00:00, 29.94it/s]\n",
            "Preparing transaction: - \b\b\\ \b\b| \b\bdone\n",
            "Verifying transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Executing transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "#\n",
            "# To activate this environment, use\n",
            "#\n",
            "#     $ conda activate timesformer\n",
            "#\n",
            "# To deactivate an active environment, use\n",
            "#\n",
            "#     $ conda deactivate\n",
            "\n",
            "\u001b[K     |████████████████████████████████| 21.0 MB 15.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 74.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 750.6 MB 10 kB/s \n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 57.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 14.6 MB/s \n",
            "\u001b[?25h  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 130 kB 14.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 431 kB 17.1 MB/s \n",
            "\u001b[?25hCollecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - av\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    aom-3.3.0                  |       h27087fc_1         2.6 MB  conda-forge\n",
            "    av-9.2.0                   |   py37he3ae868_0         979 KB  conda-forge\n",
            "    certifi-2021.10.8          |   py37h89c1867_2         145 KB  conda-forge\n",
            "    conda-4.12.0               |   py37h89c1867_0         1.0 MB  conda-forge\n",
            "    ffmpeg-4.4.1               |       h594f047_3         9.9 MB  conda-forge\n",
            "    freetype-2.10.4            |       h0708190_1         890 KB  conda-forge\n",
            "    gmp-6.2.1                  |       h58526e2_0         806 KB  conda-forge\n",
            "    gnutls-3.6.13              |       h85f3911_1         2.0 MB  conda-forge\n",
            "    icu-70.1                   |       h27087fc_0        13.5 MB  conda-forge\n",
            "    jpeg-9e                    |       h166bdaf_1         268 KB  conda-forge\n",
            "    lame-3.100                 |    h7f98852_1001         496 KB  conda-forge\n",
            "    libblas-3.9.0              |14_linux64_openblas          12 KB  conda-forge\n",
            "    libcblas-3.9.0             |14_linux64_openblas          12 KB  conda-forge\n",
            "    libdrm-2.4.109             |       h7f98852_0         284 KB  conda-forge\n",
            "    libgfortran-ng-11.2.0      |      h69a702a_16          23 KB  conda-forge\n",
            "    libgfortran5-11.2.0        |      h5c6108e_16         1.7 MB  conda-forge\n",
            "    liblapack-3.9.0            |14_linux64_openblas          12 KB  conda-forge\n",
            "    libopenblas-0.3.20         |pthreads_h78a6416_0        10.1 MB  conda-forge\n",
            "    libpciaccess-0.16          |       h516909a_0          37 KB  conda-forge\n",
            "    libpng-1.6.37              |       h21135ba_2         306 KB  conda-forge\n",
            "    libtiff-4.0.10             |    hc3755c2_1005         602 KB  conda-forge\n",
            "    libva-2.14.0               |       h7f98852_0         183 KB  conda-forge\n",
            "    libvpx-1.11.0              |       h9c3ff4c_3         1.1 MB  conda-forge\n",
            "    libxcb-1.13                |    h7f98852_1004         391 KB  conda-forge\n",
            "    libxml2-2.9.14             |       h22db469_0         770 KB  conda-forge\n",
            "    nettle-3.6                 |       he412f7d_0         6.5 MB  conda-forge\n",
            "    numpy-1.21.6               |   py37h976b520_0         6.1 MB  conda-forge\n",
            "    olefile-0.46               |     pyh9f0ad1d_1          32 KB  conda-forge\n",
            "    openh264-2.1.1             |       h780b84a_0         1.5 MB  conda-forge\n",
            "    openssl-1.1.1o             |       h166bdaf_0         2.1 MB  conda-forge\n",
            "    pillow-6.2.1               |   py37h6b7be26_0         637 KB  conda-forge\n",
            "    pthread-stubs-0.4          |    h36c2ea0_1001           5 KB  conda-forge\n",
            "    svt-av1-0.9.1              |       h27087fc_0         3.3 MB  conda-forge\n",
            "    x264-1!161.3030            |       h7f98852_1         2.5 MB  conda-forge\n",
            "    x265-3.5                   |       h924138e_3         3.2 MB  conda-forge\n",
            "    xorg-fixesproto-5.0        |    h7f98852_1002           9 KB  conda-forge\n",
            "    xorg-kbproto-1.0.7         |    h7f98852_1002          27 KB  conda-forge\n",
            "    xorg-libx11-1.7.2          |       h7f98852_0         941 KB  conda-forge\n",
            "    xorg-libxau-1.0.9          |       h7f98852_0          13 KB  conda-forge\n",
            "    xorg-libxdmcp-1.1.3        |       h7f98852_0          19 KB  conda-forge\n",
            "    xorg-libxext-1.3.4         |       h7f98852_1          54 KB  conda-forge\n",
            "    xorg-libxfixes-5.0.3       |    h7f98852_1004          18 KB  conda-forge\n",
            "    xorg-xextproto-7.3.0       |    h7f98852_1002          28 KB  conda-forge\n",
            "    xorg-xproto-7.0.31         |    h7f98852_1007          73 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        75.0 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  aom                conda-forge/linux-64::aom-3.3.0-h27087fc_1\n",
            "  av                 conda-forge/linux-64::av-9.2.0-py37he3ae868_0\n",
            "  ffmpeg             conda-forge/linux-64::ffmpeg-4.4.1-h594f047_3\n",
            "  freetype           conda-forge/linux-64::freetype-2.10.4-h0708190_1\n",
            "  gmp                conda-forge/linux-64::gmp-6.2.1-h58526e2_0\n",
            "  gnutls             conda-forge/linux-64::gnutls-3.6.13-h85f3911_1\n",
            "  jpeg               conda-forge/linux-64::jpeg-9e-h166bdaf_1\n",
            "  lame               conda-forge/linux-64::lame-3.100-h7f98852_1001\n",
            "  libblas            conda-forge/linux-64::libblas-3.9.0-14_linux64_openblas\n",
            "  libcblas           conda-forge/linux-64::libcblas-3.9.0-14_linux64_openblas\n",
            "  libdrm             conda-forge/linux-64::libdrm-2.4.109-h7f98852_0\n",
            "  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-11.2.0-h69a702a_16\n",
            "  libgfortran5       conda-forge/linux-64::libgfortran5-11.2.0-h5c6108e_16\n",
            "  liblapack          conda-forge/linux-64::liblapack-3.9.0-14_linux64_openblas\n",
            "  libopenblas        conda-forge/linux-64::libopenblas-0.3.20-pthreads_h78a6416_0\n",
            "  libpciaccess       conda-forge/linux-64::libpciaccess-0.16-h516909a_0\n",
            "  libpng             conda-forge/linux-64::libpng-1.6.37-h21135ba_2\n",
            "  libtiff            conda-forge/linux-64::libtiff-4.0.10-hc3755c2_1005\n",
            "  libva              conda-forge/linux-64::libva-2.14.0-h7f98852_0\n",
            "  libvpx             conda-forge/linux-64::libvpx-1.11.0-h9c3ff4c_3\n",
            "  libxcb             conda-forge/linux-64::libxcb-1.13-h7f98852_1004\n",
            "  libzlib            conda-forge/linux-64::libzlib-1.2.11-h166bdaf_1014\n",
            "  nettle             conda-forge/linux-64::nettle-3.6-he412f7d_0\n",
            "  numpy              conda-forge/linux-64::numpy-1.21.6-py37h976b520_0\n",
            "  olefile            conda-forge/noarch::olefile-0.46-pyh9f0ad1d_1\n",
            "  openh264           conda-forge/linux-64::openh264-2.1.1-h780b84a_0\n",
            "  pillow             conda-forge/linux-64::pillow-6.2.1-py37h6b7be26_0\n",
            "  pthread-stubs      conda-forge/linux-64::pthread-stubs-0.4-h36c2ea0_1001\n",
            "  svt-av1            conda-forge/linux-64::svt-av1-0.9.1-h27087fc_0\n",
            "  x264               conda-forge/linux-64::x264-1!161.3030-h7f98852_1\n",
            "  x265               conda-forge/linux-64::x265-3.5-h924138e_3\n",
            "  xorg-fixesproto    conda-forge/linux-64::xorg-fixesproto-5.0-h7f98852_1002\n",
            "  xorg-kbproto       conda-forge/linux-64::xorg-kbproto-1.0.7-h7f98852_1002\n",
            "  xorg-libx11        conda-forge/linux-64::xorg-libx11-1.7.2-h7f98852_0\n",
            "  xorg-libxau        conda-forge/linux-64::xorg-libxau-1.0.9-h7f98852_0\n",
            "  xorg-libxdmcp      conda-forge/linux-64::xorg-libxdmcp-1.1.3-h7f98852_0\n",
            "  xorg-libxext       conda-forge/linux-64::xorg-libxext-1.3.4-h7f98852_1\n",
            "  xorg-libxfixes     conda-forge/linux-64::xorg-libxfixes-5.0.3-h7f98852_1004\n",
            "  xorg-xextproto     conda-forge/linux-64::xorg-xextproto-7.3.0-h7f98852_1002\n",
            "  xorg-xproto        conda-forge/linux-64::xorg-xproto-7.0.31-h7f98852_1007\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates                      2020.12.5-ha878542_0 --> 2021.10.8-ha878542_0\n",
            "  certifi                          2020.12.5-py37h89c1867_1 --> 2021.10.8-py37h89c1867_2\n",
            "  conda                                4.9.2-py37h89c1867_0 --> 4.12.0-py37h89c1867_0\n",
            "  icu                                       68.1-h58526e2_0 --> 70.1-h27087fc_0\n",
            "  libgcc-ng                               9.3.0-h2828fa1_18 --> 11.2.0-h1d223b6_16\n",
            "  libgomp                                 9.3.0-h2828fa1_18 --> 11.2.0-h1d223b6_16\n",
            "  libstdcxx-ng                            9.3.0-h6de172a_18 --> 11.2.0-he4da1e4_16\n",
            "  libxml2                                 2.9.10-h72842e0_3 --> 2.9.14-h22db469_0\n",
            "  openssl                                 1.1.1j-h7f98852_0 --> 1.1.1o-h166bdaf_0\n",
            "  python_abi                                    3.7-1_cp37m --> 3.7-2_cp37m\n",
            "  zlib                                 1.2.11-h516909a_1010 --> 1.2.11-h166bdaf_1014\n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "\u001b[K     |████████████████████████████████| 280 kB 14.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 24.8 MB 14.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 306 kB 77.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 38.1 MB 133.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 60.5 MB 54 kB/s \n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 16.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 97 kB 8.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 156 kB 91.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 224 kB 90.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 80.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 126 kB 85.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 781 kB 88.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 85.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 75.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 155 kB 87.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 77 kB 6.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 151 kB 98.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 11.2 MB 14.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 40 kB 6.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 80.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 930 kB 81.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 247 kB 87.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 98 kB 8.9 MB/s \n",
            "\u001b[?25hCollecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - ipykernel\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    backcall-0.2.0             |     pyh9f0ad1d_0          13 KB  conda-forge\n",
            "    backports-1.0              |             py_2           4 KB  conda-forge\n",
            "    backports.functools_lru_cache-1.6.4|     pyhd8ed1ab_0           9 KB  conda-forge\n",
            "    debugpy-1.6.0              |   py37hd23a5d3_0         2.0 MB  conda-forge\n",
            "    decorator-5.1.1            |     pyhd8ed1ab_0          12 KB  conda-forge\n",
            "    entrypoints-0.4            |     pyhd8ed1ab_0           9 KB  conda-forge\n",
            "    ipykernel-6.13.0           |   py37h25bab4e_0         185 KB  conda-forge\n",
            "    ipython-7.33.0             |   py37h89c1867_0         1.1 MB  conda-forge\n",
            "    jedi-0.18.1                |   py37h89c1867_1        1008 KB  conda-forge\n",
            "    jupyter_client-7.3.0       |     pyhd8ed1ab_0          90 KB  conda-forge\n",
            "    jupyter_core-4.9.2         |   py37h89c1867_0          80 KB  conda-forge\n",
            "    libsodium-1.0.18           |       h36c2ea0_1         366 KB  conda-forge\n",
            "    matplotlib-inline-0.1.3    |     pyhd8ed1ab_0          11 KB  conda-forge\n",
            "    nest-asyncio-1.5.5         |     pyhd8ed1ab_0           9 KB  conda-forge\n",
            "    packaging-21.3             |     pyhd8ed1ab_0          36 KB  conda-forge\n",
            "    parso-0.8.3                |     pyhd8ed1ab_0          69 KB  conda-forge\n",
            "    pexpect-4.8.0              |     pyh9f0ad1d_2          47 KB  conda-forge\n",
            "    pickleshare-0.7.5          |          py_1003           9 KB  conda-forge\n",
            "    prompt-toolkit-3.0.29      |     pyha770c72_0         252 KB  conda-forge\n",
            "    psutil-5.9.0               |   py37h540881e_1         342 KB  conda-forge\n",
            "    ptyprocess-0.7.0           |     pyhd3deb0d_0          16 KB  conda-forge\n",
            "    pygments-2.12.0            |     pyhd8ed1ab_0         817 KB  conda-forge\n",
            "    pyparsing-3.0.8            |     pyhd8ed1ab_0          79 KB  conda-forge\n",
            "    python-dateutil-2.8.2      |     pyhd8ed1ab_0         240 KB  conda-forge\n",
            "    pyzmq-22.3.0               |   py37h0c0c2a8_2         512 KB  conda-forge\n",
            "    tornado-6.1                |   py37h540881e_3         646 KB  conda-forge\n",
            "    traitlets-5.1.1            |     pyhd8ed1ab_0          82 KB  conda-forge\n",
            "    wcwidth-0.2.5              |     pyh9f0ad1d_2          33 KB  conda-forge\n",
            "    zeromq-4.3.4               |       h9c3ff4c_1         351 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:         8.3 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  backcall           conda-forge/noarch::backcall-0.2.0-pyh9f0ad1d_0\n",
            "  backports          conda-forge/noarch::backports-1.0-py_2\n",
            "  backports.functoo~ conda-forge/noarch::backports.functools_lru_cache-1.6.4-pyhd8ed1ab_0\n",
            "  debugpy            conda-forge/linux-64::debugpy-1.6.0-py37hd23a5d3_0\n",
            "  decorator          conda-forge/noarch::decorator-5.1.1-pyhd8ed1ab_0\n",
            "  entrypoints        conda-forge/noarch::entrypoints-0.4-pyhd8ed1ab_0\n",
            "  ipykernel          conda-forge/linux-64::ipykernel-6.13.0-py37h25bab4e_0\n",
            "  ipython            conda-forge/linux-64::ipython-7.33.0-py37h89c1867_0\n",
            "  jedi               conda-forge/linux-64::jedi-0.18.1-py37h89c1867_1\n",
            "  jupyter_client     conda-forge/noarch::jupyter_client-7.3.0-pyhd8ed1ab_0\n",
            "  jupyter_core       conda-forge/linux-64::jupyter_core-4.9.2-py37h89c1867_0\n",
            "  libsodium          conda-forge/linux-64::libsodium-1.0.18-h36c2ea0_1\n",
            "  matplotlib-inline  conda-forge/noarch::matplotlib-inline-0.1.3-pyhd8ed1ab_0\n",
            "  nest-asyncio       conda-forge/noarch::nest-asyncio-1.5.5-pyhd8ed1ab_0\n",
            "  packaging          conda-forge/noarch::packaging-21.3-pyhd8ed1ab_0\n",
            "  parso              conda-forge/noarch::parso-0.8.3-pyhd8ed1ab_0\n",
            "  pexpect            conda-forge/noarch::pexpect-4.8.0-pyh9f0ad1d_2\n",
            "  pickleshare        conda-forge/noarch::pickleshare-0.7.5-py_1003\n",
            "  prompt-toolkit     conda-forge/noarch::prompt-toolkit-3.0.29-pyha770c72_0\n",
            "  psutil             conda-forge/linux-64::psutil-5.9.0-py37h540881e_1\n",
            "  ptyprocess         conda-forge/noarch::ptyprocess-0.7.0-pyhd3deb0d_0\n",
            "  pygments           conda-forge/noarch::pygments-2.12.0-pyhd8ed1ab_0\n",
            "  pyparsing          conda-forge/noarch::pyparsing-3.0.8-pyhd8ed1ab_0\n",
            "  python-dateutil    conda-forge/noarch::python-dateutil-2.8.2-pyhd8ed1ab_0\n",
            "  pyzmq              conda-forge/linux-64::pyzmq-22.3.0-py37h0c0c2a8_2\n",
            "  tornado            conda-forge/linux-64::tornado-6.1-py37h540881e_3\n",
            "  traitlets          conda-forge/noarch::traitlets-5.1.1-pyhd8ed1ab_0\n",
            "  wcwidth            conda-forge/noarch::wcwidth-0.2.5-pyh9f0ad1d_2\n",
            "  zeromq             conda-forge/linux-64::zeromq-4.3.4-h9c3ff4c_1\n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "Collecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - ipython_genutils\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    ipython_genutils-0.2.0     |             py_1          21 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:          21 KB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  ipython_genutils   conda-forge/noarch::ipython_genutils-0.2.0-py_1\n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 20.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 72 kB 1.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 95 kB 4.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 133 kB 95.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 96 kB 6.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 117 kB 77.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 60 kB 9.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 503 kB 74.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.5 MB/s \n",
            "\u001b[?25hCloning into 'TimeSformer'...\n",
            "remote: Enumerating objects: 446, done.\u001b[K\n",
            "remote: Counting objects: 100% (123/123), done.\u001b[K\n",
            "remote: Compressing objects: 100% (62/62), done.\u001b[K\n",
            "remote: Total 446 (delta 92), reused 61 (delta 61), pack-reused 323\u001b[K\n",
            "Receiving objects: 100% (446/446), 362.94 KiB | 919.00 KiB/s, done.\n",
            "Resolving deltas: 100% (207/207), done.\n",
            "/content/TimeSformer\n",
            "running build\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/timesformer\n",
            "copying timesformer/__init__.py -> build/lib/timesformer\n",
            "creating build/lib/timesformer/config\n",
            "copying timesformer/config/__init__.py -> build/lib/timesformer/config\n",
            "copying timesformer/config/defaults.py -> build/lib/timesformer/config\n",
            "creating build/lib/timesformer/datasets\n",
            "copying timesformer/datasets/utils.py -> build/lib/timesformer/datasets\n",
            "copying timesformer/datasets/ssv2.py -> build/lib/timesformer/datasets\n",
            "copying timesformer/datasets/cv2_transform.py -> build/lib/timesformer/datasets\n",
            "copying timesformer/datasets/video_container.py -> build/lib/timesformer/datasets\n",
            "copying timesformer/datasets/transform.py -> build/lib/timesformer/datasets\n",
            "copying timesformer/datasets/build.py -> build/lib/timesformer/datasets\n",
            "copying timesformer/datasets/multigrid_helper.py -> build/lib/timesformer/datasets\n",
            "copying timesformer/datasets/__init__.py -> build/lib/timesformer/datasets\n",
            "copying timesformer/datasets/loader.py -> build/lib/timesformer/datasets\n",
            "copying timesformer/datasets/kinetics.py -> build/lib/timesformer/datasets\n",
            "copying timesformer/datasets/decoder.py -> build/lib/timesformer/datasets\n",
            "creating build/lib/timesformer/visualization\n",
            "copying timesformer/visualization/utils.py -> build/lib/timesformer/visualization\n",
            "copying timesformer/visualization/__init__.py -> build/lib/timesformer/visualization\n",
            "copying timesformer/visualization/tensorboard_vis.py -> build/lib/timesformer/visualization\n",
            "creating build/lib/timesformer/utils\n",
            "copying timesformer/utils/lr_policy.py -> build/lib/timesformer/utils\n",
            "copying timesformer/utils/c2_model_loading.py -> build/lib/timesformer/utils\n",
            "copying timesformer/utils/ava_eval_helper.py -> build/lib/timesformer/utils\n",
            "copying timesformer/utils/misc.py -> build/lib/timesformer/utils\n",
            "copying timesformer/utils/env.py -> build/lib/timesformer/utils\n",
            "copying timesformer/utils/checkpoint.py -> build/lib/timesformer/utils\n",
            "copying timesformer/utils/distributed.py -> build/lib/timesformer/utils\n",
            "copying timesformer/utils/metrics.py -> build/lib/timesformer/utils\n",
            "copying timesformer/utils/bn_helper.py -> build/lib/timesformer/utils\n",
            "copying timesformer/utils/benchmark.py -> build/lib/timesformer/utils\n",
            "copying timesformer/utils/weight_init_helper.py -> build/lib/timesformer/utils\n",
            "copying timesformer/utils/__init__.py -> build/lib/timesformer/utils\n",
            "copying timesformer/utils/multigrid.py -> build/lib/timesformer/utils\n",
            "copying timesformer/utils/logging.py -> build/lib/timesformer/utils\n",
            "copying timesformer/utils/meters.py -> build/lib/timesformer/utils\n",
            "copying timesformer/utils/multiprocessing.py -> build/lib/timesformer/utils\n",
            "copying timesformer/utils/parser.py -> build/lib/timesformer/utils\n",
            "creating build/lib/timesformer/models\n",
            "copying timesformer/models/stem_helper.py -> build/lib/timesformer/models\n",
            "copying timesformer/models/features.py -> build/lib/timesformer/models\n",
            "copying timesformer/models/optimizer.py -> build/lib/timesformer/models\n",
            "copying timesformer/models/nonlocal_helper.py -> build/lib/timesformer/models\n",
            "copying timesformer/models/head_helper.py -> build/lib/timesformer/models\n",
            "copying timesformer/models/losses.py -> build/lib/timesformer/models\n",
            "copying timesformer/models/conv2d_same.py -> build/lib/timesformer/models\n",
            "copying timesformer/models/batchnorm_helper.py -> build/lib/timesformer/models\n",
            "copying timesformer/models/custom_video_model_builder.py -> build/lib/timesformer/models\n",
            "copying timesformer/models/build.py -> build/lib/timesformer/models\n",
            "copying timesformer/models/vit_utils.py -> build/lib/timesformer/models\n",
            "copying timesformer/models/__init__.py -> build/lib/timesformer/models\n",
            "copying timesformer/models/vit.py -> build/lib/timesformer/models\n",
            "copying timesformer/models/operators.py -> build/lib/timesformer/models\n",
            "copying timesformer/models/linear.py -> build/lib/timesformer/models\n",
            "copying timesformer/models/helpers.py -> build/lib/timesformer/models\n",
            "copying timesformer/models/resnet_helper.py -> build/lib/timesformer/models\n",
            "copying timesformer/models/video_model_builder.py -> build/lib/timesformer/models\n",
            "creating build/lib/timesformer/utils/ava_evaluation\n",
            "copying timesformer/utils/ava_evaluation/np_box_list_ops.py -> build/lib/timesformer/utils/ava_evaluation\n",
            "copying timesformer/utils/ava_evaluation/np_box_mask_list.py -> build/lib/timesformer/utils/ava_evaluation\n",
            "copying timesformer/utils/ava_evaluation/np_mask_ops.py -> build/lib/timesformer/utils/ava_evaluation\n",
            "copying timesformer/utils/ava_evaluation/standard_fields.py -> build/lib/timesformer/utils/ava_evaluation\n",
            "copying timesformer/utils/ava_evaluation/np_box_ops.py -> build/lib/timesformer/utils/ava_evaluation\n",
            "copying timesformer/utils/ava_evaluation/metrics.py -> build/lib/timesformer/utils/ava_evaluation\n",
            "copying timesformer/utils/ava_evaluation/np_box_mask_list_ops.py -> build/lib/timesformer/utils/ava_evaluation\n",
            "copying timesformer/utils/ava_evaluation/__init__.py -> build/lib/timesformer/utils/ava_evaluation\n",
            "copying timesformer/utils/ava_evaluation/label_map_util.py -> build/lib/timesformer/utils/ava_evaluation\n",
            "copying timesformer/utils/ava_evaluation/per_image_evaluation.py -> build/lib/timesformer/utils/ava_evaluation\n",
            "copying timesformer/utils/ava_evaluation/object_detection_evaluation.py -> build/lib/timesformer/utils/ava_evaluation\n",
            "copying timesformer/utils/ava_evaluation/np_box_list.py -> build/lib/timesformer/utils/ava_evaluation\n",
            "running develop\n",
            "running egg_info\n",
            "creating timesformer.egg-info\n",
            "writing timesformer.egg-info/PKG-INFO\n",
            "writing dependency_links to timesformer.egg-info/dependency_links.txt\n",
            "writing requirements to timesformer.egg-info/requires.txt\n",
            "writing top-level names to timesformer.egg-info/top_level.txt\n",
            "writing manifest file 'timesformer.egg-info/SOURCES.txt'\n",
            "reading manifest file 'timesformer.egg-info/SOURCES.txt'\n",
            "writing manifest file 'timesformer.egg-info/SOURCES.txt'\n",
            "running build_ext\n",
            "Creating /usr/local/lib/python3.7/site-packages/timesformer.egg-link (link to .)\n",
            "Adding timesformer 1.0 to easy-install.pth file\n",
            "\n",
            "Installed /content/TimeSformer\n",
            "Processing dependencies for timesformer==1.0\n",
            "Searching for torch==1.11.0\n",
            "Best match: torch 1.11.0\n",
            "Adding torch 1.11.0 to easy-install.pth file\n",
            "Installing convert-caffe2-to-onnx script to /usr/local/bin\n",
            "Installing convert-onnx-to-caffe2 script to /usr/local/bin\n",
            "Installing torchrun script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for einops==0.4.1\n",
            "Best match: einops 0.4.1\n",
            "Adding einops 0.4.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for typing-extensions==4.2.0\n",
            "Best match: typing-extensions 4.2.0\n",
            "Adding typing-extensions 4.2.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Finished processing dependencies for timesformer==1.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/TimeSformer/checkpoints/checkpoint_epoch_00015.pyth'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell will set up the cofigurations of the TimesFormer"
      ],
      "metadata": {
        "id": "Bu6ZF2ZI53tW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the default config file (added this here so that changes like prediction results save path or load from checkpoint can be easily made)\n",
        "\n",
        "%%writefile /content/TimeSformer/timesformer/config/defaults.py \n",
        "\n",
        "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n",
        "\n",
        "\"\"\"Configs.\"\"\"\n",
        "from fvcore.common.config import CfgNode\n",
        "# -----------------------------------------------------------------------------\n",
        "# Config definition\n",
        "# -----------------------------------------------------------------------------\n",
        "_C = CfgNode()\n",
        "\n",
        "# ---------------------------------------------------------------------------- #\n",
        "# Batch norm options\n",
        "# ---------------------------------------------------------------------------- #\n",
        "_C.BN = CfgNode()\n",
        "\n",
        "# Precise BN stats.\n",
        "_C.BN.USE_PRECISE_STATS = False\n",
        "\n",
        "# Number of samples use to compute precise bn.\n",
        "_C.BN.NUM_BATCHES_PRECISE = 200\n",
        "\n",
        "# Weight decay value that applies on BN.\n",
        "_C.BN.WEIGHT_DECAY = 0.0\n",
        "\n",
        "# Norm type, options include `batchnorm`, `sub_batchnorm`, `sync_batchnorm`\n",
        "_C.BN.NORM_TYPE = \"batchnorm\"\n",
        "\n",
        "# Parameter for SubBatchNorm, where it splits the batch dimension into\n",
        "# NUM_SPLITS splits, and run BN on each of them separately independently.\n",
        "_C.BN.NUM_SPLITS = 1\n",
        "\n",
        "# Parameter for NaiveSyncBatchNorm3d, where the stats across `NUM_SYNC_DEVICES`\n",
        "# devices will be synchronized.\n",
        "_C.BN.NUM_SYNC_DEVICES = 1\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------- #\n",
        "# Training options.\n",
        "# ---------------------------------------------------------------------------- #\n",
        "_C.TRAIN = CfgNode()\n",
        "\n",
        "# If True Train the model, else skip training.\n",
        "_C.TRAIN.ENABLE = True\n",
        "\n",
        "# Dataset.\n",
        "_C.TRAIN.DATASET = \"kinetics\"\n",
        "\n",
        "##\n",
        "_C.TRAIN.FINETUNE = False\n",
        "\n",
        "# Total mini-batch size.\n",
        "_C.TRAIN.BATCH_SIZE = 64\n",
        "\n",
        "# Evaluate model on test data every eval period epochs.\n",
        "_C.TRAIN.EVAL_PERIOD = 10\n",
        "\n",
        "# Save model checkpoint every checkpoint period epochs.\n",
        "_C.TRAIN.CHECKPOINT_PERIOD = 10\n",
        "\n",
        "# Resume training from the latest checkpoint in the output directory.\n",
        "_C.TRAIN.AUTO_RESUME = True\n",
        "\n",
        "# Path to the checkpoint to load the initial weight.\n",
        "_C.TRAIN.CHECKPOINT_FILE_PATH = \"\"\n",
        "\n",
        "# Checkpoint types include `caffe2` or `pytorch`.\n",
        "_C.TRAIN.CHECKPOINT_TYPE = \"pytorch\"\n",
        "\n",
        "# If True, perform inflation when loading checkpoint.\n",
        "_C.TRAIN.CHECKPOINT_INFLATE = False\n",
        "\n",
        "# If True, reset epochs when loading checkpoint.\n",
        "_C.TRAIN.CHECKPOINT_EPOCH_RESET = False\n",
        "\n",
        "# If set, clear all layer names according to the pattern provided.\n",
        "_C.TRAIN.CHECKPOINT_CLEAR_NAME_PATTERN = ()  # (\"backbone.\",)\n",
        "\n",
        "# ---------------------------------------------------------------------------- #\n",
        "# Testing options\n",
        "# ---------------------------------------------------------------------------- #\n",
        "_C.TEST = CfgNode()\n",
        "\n",
        "# If True test the model, else skip the testing.\n",
        "_C.TEST.ENABLE = True\n",
        "\n",
        "# Dataset for testing.\n",
        "_C.TEST.DATASET = \"kinetics\"\n",
        "\n",
        "# Total mini-batch size\n",
        "_C.TEST.BATCH_SIZE = 8\n",
        "\n",
        "# Path to the checkpoint to load the initial weight.\n",
        "_C.TEST.CHECKPOINT_FILE_PATH = \"\"\n",
        "\n",
        "# Number of clips to sample from a video uniformly for aggregating the\n",
        "# prediction results.\n",
        "_C.TEST.NUM_ENSEMBLE_VIEWS = 10\n",
        "\n",
        "# Number of crops to sample from a frame spatially for aggregating the\n",
        "# prediction results.\n",
        "_C.TEST.NUM_SPATIAL_CROPS = 3\n",
        "\n",
        "# Checkpoint types include `caffe2` or `pytorch`.\n",
        "_C.TEST.CHECKPOINT_TYPE = \"pytorch\"\n",
        "# Path to saving prediction results file.\n",
        "_C.TEST.SAVE_RESULTS_PATH = \"/content/result.pkl\"\n",
        "# -----------------------------------------------------------------------------\n",
        "# ResNet options\n",
        "# -----------------------------------------------------------------------------\n",
        "_C.RESNET = CfgNode()\n",
        "\n",
        "# Transformation function.\n",
        "_C.RESNET.TRANS_FUNC = \"bottleneck_transform\"\n",
        "\n",
        "# Number of groups. 1 for ResNet, and larger than 1 for ResNeXt).\n",
        "_C.RESNET.NUM_GROUPS = 1\n",
        "\n",
        "# Width of each group (64 -> ResNet; 4 -> ResNeXt).\n",
        "_C.RESNET.WIDTH_PER_GROUP = 64\n",
        "\n",
        "# Apply relu in a inplace manner.\n",
        "_C.RESNET.INPLACE_RELU = True\n",
        "\n",
        "# Apply stride to 1x1 conv.\n",
        "_C.RESNET.STRIDE_1X1 = False\n",
        "\n",
        "#  If true, initialize the gamma of the final BN of each block to zero.\n",
        "_C.RESNET.ZERO_INIT_FINAL_BN = False\n",
        "\n",
        "# Number of weight layers.\n",
        "_C.RESNET.DEPTH = 50\n",
        "\n",
        "# If the current block has more than NUM_BLOCK_TEMP_KERNEL blocks, use temporal\n",
        "# kernel of 1 for the rest of the blocks.\n",
        "_C.RESNET.NUM_BLOCK_TEMP_KERNEL = [[3], [4], [6], [3]]\n",
        "\n",
        "# Size of stride on different res stages.\n",
        "_C.RESNET.SPATIAL_STRIDES = [[1], [2], [2], [2]]\n",
        "\n",
        "# Size of dilation on different res stages.\n",
        "_C.RESNET.SPATIAL_DILATIONS = [[1], [1], [1], [1]]\n",
        "\n",
        "# ---------------------------------------------------------------------------- #\n",
        "# X3D  options\n",
        "# See https://arxiv.org/abs/2004.04730 for details about X3D Networks.\n",
        "# ---------------------------------------------------------------------------- #\n",
        "_C.X3D = CfgNode()\n",
        "\n",
        "# Width expansion factor.\n",
        "_C.X3D.WIDTH_FACTOR = 1.0\n",
        "\n",
        "# Depth expansion factor.\n",
        "_C.X3D.DEPTH_FACTOR = 1.0\n",
        "\n",
        "# Bottleneck expansion factor for the 3x3x3 conv.\n",
        "_C.X3D.BOTTLENECK_FACTOR = 1.0  #\n",
        "\n",
        "# Dimensions of the last linear layer before classificaiton.\n",
        "_C.X3D.DIM_C5 = 2048\n",
        "\n",
        "# Dimensions of the first 3x3 conv layer.\n",
        "_C.X3D.DIM_C1 = 12\n",
        "\n",
        "# Whether to scale the width of Res2, default is false.\n",
        "_C.X3D.SCALE_RES2 = False\n",
        "\n",
        "# Whether to use a BatchNorm (BN) layer before the classifier, default is false.\n",
        "_C.X3D.BN_LIN5 = False\n",
        "\n",
        "# Whether to use channelwise (=depthwise) convolution in the center (3x3x3)\n",
        "# convolution operation of the residual blocks.\n",
        "_C.X3D.CHANNELWISE_3x3x3 = True\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Nonlocal options\n",
        "# -----------------------------------------------------------------------------\n",
        "_C.NONLOCAL = CfgNode()\n",
        "\n",
        "# Index of each stage and block to add nonlocal layers.\n",
        "_C.NONLOCAL.LOCATION = [[[]], [[]], [[]], [[]]]\n",
        "\n",
        "# Number of group for nonlocal for each stage.\n",
        "_C.NONLOCAL.GROUP = [[1], [1], [1], [1]]\n",
        "\n",
        "# Instatiation to use for non-local layer.\n",
        "_C.NONLOCAL.INSTANTIATION = \"dot_product\"\n",
        "\n",
        "\n",
        "# Size of pooling layers used in Non-Local.\n",
        "_C.NONLOCAL.POOL = [\n",
        "    # Res2\n",
        "    [[1, 2, 2], [1, 2, 2]],\n",
        "    # Res3\n",
        "    [[1, 2, 2], [1, 2, 2]],\n",
        "    # Res4\n",
        "    [[1, 2, 2], [1, 2, 2]],\n",
        "    # Res5\n",
        "    [[1, 2, 2], [1, 2, 2]],\n",
        "]\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Model options\n",
        "# -----------------------------------------------------------------------------\n",
        "_C.MODEL = CfgNode()\n",
        "\n",
        "# Model architecture.\n",
        "_C.MODEL.ARCH = \"slowfast\"\n",
        "\n",
        "# Model name\n",
        "_C.MODEL.MODEL_NAME = \"SlowFast\"\n",
        "\n",
        "# The number of classes to predict for the model.\n",
        "_C.MODEL.NUM_CLASSES = 14\n",
        "\n",
        "# Loss function.\n",
        "_C.MODEL.LOSS_FUNC = \"cross_entropy\"\n",
        "\n",
        "# Model architectures that has one single pathway.\n",
        "_C.MODEL.SINGLE_PATHWAY_ARCH = [\"c2d\", \"i3d\", \"slow\", \"x3d\"]\n",
        "\n",
        "# Model architectures that has multiple pathways.\n",
        "_C.MODEL.MULTI_PATHWAY_ARCH = [\"slowfast\"]\n",
        "\n",
        "# Dropout rate before final projection in the backbone.\n",
        "_C.MODEL.DROPOUT_RATE = 0.5\n",
        "\n",
        "# Randomly drop rate for Res-blocks, linearly increase from res2 to res5\n",
        "_C.MODEL.DROPCONNECT_RATE = 0.0\n",
        "\n",
        "# The std to initialize the fc layer(s).\n",
        "_C.MODEL.FC_INIT_STD = 0.01\n",
        "\n",
        "# Activation layer for the output head.\n",
        "_C.MODEL.HEAD_ACT = \"softmax\"\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# SlowFast options\n",
        "# -----------------------------------------------------------------------------\n",
        "_C.SLOWFAST = CfgNode()\n",
        "\n",
        "# Corresponds to the inverse of the channel reduction ratio, $\\beta$ between\n",
        "# the Slow and Fast pathways.\n",
        "_C.SLOWFAST.BETA_INV = 8\n",
        "\n",
        "# Corresponds to the frame rate reduction ratio, $\\alpha$ between the Slow and\n",
        "# Fast pathways.\n",
        "_C.SLOWFAST.ALPHA = 8\n",
        "\n",
        "# Ratio of channel dimensions between the Slow and Fast pathways.\n",
        "_C.SLOWFAST.FUSION_CONV_CHANNEL_RATIO = 2\n",
        "\n",
        "# Kernel dimension used for fusing information from Fast pathway to Slow\n",
        "# pathway.\n",
        "_C.SLOWFAST.FUSION_KERNEL_SZ = 5\n",
        "\n",
        "####### TimeSformer Options\n",
        "_C.TIMESFORMER = CfgNode()\n",
        "_C.TIMESFORMER.ATTENTION_TYPE = 'divided_space_time'\n",
        "_C.TIMESFORMER.PRETRAINED_MODEL = ''\n",
        "\n",
        "## MixUp parameters\n",
        "_C.MIXUP = CfgNode()\n",
        "_C.MIXUP.ENABLED = False\n",
        "_C.MIXUP.ALPHA = 0.8\n",
        "_C.MIXUP.CUTMIX_ALPHA = 1.0\n",
        "_C.MIXUP.CUTMIX_MINMAX = None\n",
        "_C.MIXUP.PROB = 1.0\n",
        "_C.MIXUP.SWITCH_PROB = 0.5\n",
        "_C.MIXUP.MODE = 'batch'\n",
        "\n",
        "_C.EMA = CfgNode()\n",
        "_C.EMA.ENABLED = False\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Data options\n",
        "# -----------------------------------------------------------------------------\n",
        "_C.DATA = CfgNode()\n",
        "\n",
        "# The path to the data directory.\n",
        "_C.DATA.PATH_TO_DATA_DIR = \"\"\n",
        "\n",
        "# The separator used between path and label.\n",
        "_C.DATA.PATH_LABEL_SEPARATOR = \",\"\n",
        "\n",
        "# Video path prefix if any.\n",
        "_C.DATA.PATH_PREFIX = \"\"\n",
        "\n",
        "# The spatial crop size of the input clip.\n",
        "_C.DATA.CROP_SIZE = 224\n",
        "\n",
        "# The number of frames of the input clip.\n",
        "_C.DATA.NUM_FRAMES = 8\n",
        "\n",
        "# The video sampling rate of the input clip.\n",
        "_C.DATA.SAMPLING_RATE = 8\n",
        "\n",
        "# The mean value of the video raw pixels across the R G B channels.\n",
        "_C.DATA.MEAN = [0.45, 0.45, 0.45]\n",
        "# List of input frame channel dimensions.\n",
        "\n",
        "_C.DATA.INPUT_CHANNEL_NUM = [3, 3]\n",
        "\n",
        "# The std value of the video raw pixels across the R G B channels.\n",
        "_C.DATA.STD = [0.225, 0.225, 0.225]\n",
        "\n",
        "# The spatial augmentation jitter scales for training.\n",
        "_C.DATA.TRAIN_JITTER_SCALES = [256, 320]\n",
        "\n",
        "# The spatial crop size for training.\n",
        "_C.DATA.TRAIN_CROP_SIZE = 224\n",
        "\n",
        "# The spatial crop size for testing.\n",
        "_C.DATA.TEST_CROP_SIZE = 256\n",
        "\n",
        "# Input videos may has different fps, convert it to the target video fps before\n",
        "# frame sampling.\n",
        "_C.DATA.TARGET_FPS = 30\n",
        "\n",
        "# Decoding backend, options include `pyav` or `torchvision`\n",
        "_C.DATA.DECODING_BACKEND = \"pyav\"\n",
        "\n",
        "# if True, sample uniformly in [1 / max_scale, 1 / min_scale] and take a\n",
        "# reciprocal to get the scale. If False, take a uniform sample from\n",
        "# [min_scale, max_scale].\n",
        "_C.DATA.INV_UNIFORM_SAMPLE = False\n",
        "\n",
        "# If True, perform random horizontal flip on the video frames during training.\n",
        "_C.DATA.RANDOM_FLIP = True\n",
        "\n",
        "# If True, calculdate the map as metric.\n",
        "_C.DATA.MULTI_LABEL = False\n",
        "\n",
        "# Method to perform the ensemble, options include \"sum\" and \"max\".\n",
        "_C.DATA.ENSEMBLE_METHOD = \"sum\"\n",
        "\n",
        "# If True, revert the default input channel (RBG <-> BGR).\n",
        "_C.DATA.REVERSE_INPUT_CHANNEL = False\n",
        "\n",
        "############\n",
        "_C.DATA.TEMPORAL_EXTENT = 8\n",
        "_C.DATA.DEIT_TRANSFORMS = False\n",
        "_C.DATA.COLOR_JITTER = 0.\n",
        "_C.DATA.AUTO_AUGMENT = ''\n",
        "_C.DATA.RE_PROB = 0.0\n",
        "\n",
        "# ---------------------------------------------------------------------------- #\n",
        "# Optimizer options\n",
        "# ---------------------------------------------------------------------------- #\n",
        "_C.SOLVER = CfgNode()\n",
        "\n",
        "# Base learning rate.\n",
        "_C.SOLVER.BASE_LR = 0.1\n",
        "\n",
        "# Learning rate policy (see utils/lr_policy.py for options and examples).\n",
        "_C.SOLVER.LR_POLICY = \"cosine\"\n",
        "\n",
        "# Final learning rates for 'cosine' policy.\n",
        "_C.SOLVER.COSINE_END_LR = 0.0\n",
        "\n",
        "# Exponential decay factor.\n",
        "_C.SOLVER.GAMMA = 0.1\n",
        "\n",
        "# Step size for 'exp' and 'cos' policies (in epochs).\n",
        "_C.SOLVER.STEP_SIZE = 1\n",
        "\n",
        "# Steps for 'steps_' policies (in epochs).\n",
        "_C.SOLVER.STEPS = []\n",
        "\n",
        "# Learning rates for 'steps_' policies.\n",
        "_C.SOLVER.LRS = []\n",
        "\n",
        "# Maximal number of epochs.\n",
        "_C.SOLVER.MAX_EPOCH = 300\n",
        "\n",
        "# Momentum.\n",
        "_C.SOLVER.MOMENTUM = 0.9\n",
        "\n",
        "# Momentum dampening.\n",
        "_C.SOLVER.DAMPENING = 0.0\n",
        "\n",
        "# Nesterov momentum.\n",
        "_C.SOLVER.NESTEROV = True\n",
        "\n",
        "# L2 regularization.\n",
        "_C.SOLVER.WEIGHT_DECAY = 1e-4\n",
        "\n",
        "# Start the warm up from SOLVER.BASE_LR * SOLVER.WARMUP_FACTOR.\n",
        "_C.SOLVER.WARMUP_FACTOR = 0.1\n",
        "\n",
        "# Gradually warm up the SOLVER.BASE_LR over this number of epochs.\n",
        "_C.SOLVER.WARMUP_EPOCHS = 0.0\n",
        "\n",
        "# The start learning rate of the warm up.\n",
        "_C.SOLVER.WARMUP_START_LR = 0.01\n",
        "\n",
        "# Optimization method.\n",
        "_C.SOLVER.OPTIMIZING_METHOD = \"sgd\"\n",
        "\n",
        "# Base learning rate is linearly scaled with NUM_SHARDS.\n",
        "_C.SOLVER.BASE_LR_SCALE_NUM_SHARDS = False\n",
        "\n",
        "# ---------------------------------------------------------------------------- #\n",
        "# Misc options\n",
        "# ---------------------------------------------------------------------------- #\n",
        "\n",
        "# Number of GPUs to use (applies to both training and testing).\n",
        "_C.NUM_GPUS = 1\n",
        "\n",
        "# Number of machine to use for the job.\n",
        "_C.NUM_SHARDS = 1\n",
        "\n",
        "# The index of the current machine.\n",
        "_C.SHARD_ID = 0\n",
        "\n",
        "# Output basedir.\n",
        "_C.OUTPUT_DIR = \"./tmp\"\n",
        "\n",
        "# Note that non-determinism may still be present due to non-deterministic\n",
        "# operator implementations in GPU operator libraries.\n",
        "_C.RNG_SEED = 1\n",
        "\n",
        "# Log period in iters.\n",
        "_C.LOG_PERIOD = 10\n",
        "\n",
        "# If True, log the model info.\n",
        "_C.LOG_MODEL_INFO = False\n",
        "\n",
        "# Distributed backend.\n",
        "_C.DIST_BACKEND = \"nccl\"\n",
        "\n",
        "# Global batch size\n",
        "_C.GLOBAL_BATCH_SIZE = 64\n",
        "\n",
        "# ---------------------------------------------------------------------------- #\n",
        "# Benchmark options\n",
        "# ---------------------------------------------------------------------------- #\n",
        "_C.BENCHMARK = CfgNode()\n",
        "\n",
        "# Number of epochs for data loading benchmark.\n",
        "_C.BENCHMARK.NUM_EPOCHS = 5\n",
        "\n",
        "# Log period in iters for data loading benchmark.\n",
        "_C.BENCHMARK.LOG_PERIOD = 100\n",
        "\n",
        "# If True, shuffle dataloader for epoch during benchmark.\n",
        "_C.BENCHMARK.SHUFFLE = True\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------- #\n",
        "# Common train/test data loader options\n",
        "# ---------------------------------------------------------------------------- #\n",
        "_C.DATA_LOADER = CfgNode()\n",
        "\n",
        "# Number of data loader workers per training process.\n",
        "_C.DATA_LOADER.NUM_WORKERS = 8\n",
        "\n",
        "# Load data to pinned host memory.\n",
        "_C.DATA_LOADER.PIN_MEMORY = True\n",
        "\n",
        "# Enable multi thread decoding.\n",
        "_C.DATA_LOADER.ENABLE_MULTI_THREAD_DECODE = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------- #\n",
        "# Detection options.\n",
        "# ---------------------------------------------------------------------------- #\n",
        "_C.DETECTION = CfgNode()\n",
        "\n",
        "# Whether enable video detection.\n",
        "_C.DETECTION.ENABLE = False\n",
        "\n",
        "# Aligned version of RoI. More details can be found at slowfast/models/head_helper.py\n",
        "_C.DETECTION.ALIGNED = True\n",
        "\n",
        "# Spatial scale factor.\n",
        "_C.DETECTION.SPATIAL_SCALE_FACTOR = 16\n",
        "\n",
        "# RoI tranformation resolution.\n",
        "_C.DETECTION.ROI_XFORM_RESOLUTION = 7\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# AVA Dataset options\n",
        "# -----------------------------------------------------------------------------\n",
        "_C.AVA = CfgNode()\n",
        "\n",
        "# Directory path of frames.\n",
        "_C.AVA.FRAME_DIR = \"/mnt/fair-flash3-east/ava_trainval_frames.img/\"\n",
        "\n",
        "# Directory path for files of frame lists.\n",
        "_C.AVA.FRAME_LIST_DIR = (\n",
        "    \"/mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/\"\n",
        ")\n",
        "\n",
        "# Directory path for annotation files.\n",
        "_C.AVA.ANNOTATION_DIR = (\n",
        "    \"/mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/\"\n",
        ")\n",
        "\n",
        "# Filenames of training samples list files.\n",
        "_C.AVA.TRAIN_LISTS = [\"train.csv\"]\n",
        "\n",
        "# Filenames of test samples list files.\n",
        "_C.AVA.TEST_LISTS = [\"val.csv\"]\n",
        "\n",
        "# Filenames of box list files for training. Note that we assume files which\n",
        "# contains predicted boxes will have a suffix \"predicted_boxes\" in the\n",
        "# filename.\n",
        "_C.AVA.TRAIN_GT_BOX_LISTS = [\"ava_train_v2.2.csv\"]\n",
        "_C.AVA.TRAIN_PREDICT_BOX_LISTS = []\n",
        "\n",
        "# Filenames of box list files for test.\n",
        "_C.AVA.TEST_PREDICT_BOX_LISTS = [\"ava_val_predicted_boxes.csv\"]\n",
        "\n",
        "# This option controls the score threshold for the predicted boxes to use.\n",
        "_C.AVA.DETECTION_SCORE_THRESH = 0.9\n",
        "\n",
        "# If use BGR as the format of input frames.\n",
        "_C.AVA.BGR = False\n",
        "\n",
        "# Training augmentation parameters\n",
        "# Whether to use color augmentation method.\n",
        "_C.AVA.TRAIN_USE_COLOR_AUGMENTATION = False\n",
        "\n",
        "# Whether to only use PCA jitter augmentation when using color augmentation\n",
        "# method (otherwise combine with color jitter method).\n",
        "_C.AVA.TRAIN_PCA_JITTER_ONLY = True\n",
        "\n",
        "# Eigenvalues for PCA jittering. Note PCA is RGB based.\n",
        "_C.AVA.TRAIN_PCA_EIGVAL = [0.225, 0.224, 0.229]\n",
        "\n",
        "# Eigenvectors for PCA jittering.\n",
        "_C.AVA.TRAIN_PCA_EIGVEC = [\n",
        "    [-0.5675, 0.7192, 0.4009],\n",
        "    [-0.5808, -0.0045, -0.8140],\n",
        "    [-0.5836, -0.6948, 0.4203],\n",
        "]\n",
        "\n",
        "# Whether to do horizontal flipping during test.\n",
        "_C.AVA.TEST_FORCE_FLIP = False\n",
        "\n",
        "# Whether to use full test set for validation split.\n",
        "_C.AVA.FULL_TEST_ON_VAL = False\n",
        "\n",
        "# The name of the file to the ava label map.\n",
        "_C.AVA.LABEL_MAP_FILE = \"ava_action_list_v2.2_for_activitynet_2019.pbtxt\"\n",
        "\n",
        "# The name of the file to the ava exclusion.\n",
        "_C.AVA.EXCLUSION_FILE = \"ava_val_excluded_timestamps_v2.2.csv\"\n",
        "\n",
        "# The name of the file to the ava groundtruth.\n",
        "_C.AVA.GROUNDTRUTH_FILE = \"ava_val_v2.2.csv\"\n",
        "\n",
        "# Backend to process image, includes `pytorch` and `cv2`.\n",
        "_C.AVA.IMG_PROC_BACKEND = \"cv2\"\n",
        "\n",
        "# ---------------------------------------------------------------------------- #\n",
        "# Multigrid training options\n",
        "# See https://arxiv.org/abs/1912.00998 for details about multigrid training.\n",
        "# ---------------------------------------------------------------------------- #\n",
        "_C.MULTIGRID = CfgNode()\n",
        "\n",
        "# Multigrid training allows us to train for more epochs with fewer iterations.\n",
        "# This hyperparameter specifies how many times more epochs to train.\n",
        "# The default setting in paper trains for 1.5x more epochs than baseline.\n",
        "_C.MULTIGRID.EPOCH_FACTOR = 1.5\n",
        "\n",
        "# Enable short cycles.\n",
        "_C.MULTIGRID.SHORT_CYCLE = False\n",
        "# Short cycle additional spatial dimensions relative to the default crop size.\n",
        "_C.MULTIGRID.SHORT_CYCLE_FACTORS = [0.5, 0.5 ** 0.5]\n",
        "\n",
        "_C.MULTIGRID.LONG_CYCLE = False\n",
        "# (Temporal, Spatial) dimensions relative to the default shape.\n",
        "_C.MULTIGRID.LONG_CYCLE_FACTORS = [\n",
        "    (0.25, 0.5 ** 0.5),\n",
        "    (0.5, 0.5 ** 0.5),\n",
        "    (0.5, 1),\n",
        "    (1, 1),\n",
        "]\n",
        "\n",
        "# While a standard BN computes stats across all examples in a GPU,\n",
        "# for multigrid training we fix the number of clips to compute BN stats on.\n",
        "# See https://arxiv.org/abs/1912.00998 for details.\n",
        "_C.MULTIGRID.BN_BASE_SIZE = 8\n",
        "\n",
        "# Multigrid training epochs are not proportional to actual training time or\n",
        "# computations, so _C.TRAIN.EVAL_PERIOD leads to too frequent or rare\n",
        "# evaluation. We use a multigrid-specific rule to determine when to evaluate:\n",
        "# This hyperparameter defines how many times to evaluate a model per long\n",
        "# cycle shape.\n",
        "_C.MULTIGRID.EVAL_FREQ = 3\n",
        "\n",
        "# No need to specify; Set automatically and used as global variables.\n",
        "_C.MULTIGRID.LONG_CYCLE_SAMPLING_RATE = 0\n",
        "_C.MULTIGRID.DEFAULT_B = 0\n",
        "_C.MULTIGRID.DEFAULT_T = 0\n",
        "_C.MULTIGRID.DEFAULT_S = 0\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Tensorboard Visualization Options\n",
        "# -----------------------------------------------------------------------------\n",
        "_C.TENSORBOARD = CfgNode()\n",
        "\n",
        "# Log to summary writer, this will automatically.\n",
        "# log loss, lr and metrics during train/eval.\n",
        "_C.TENSORBOARD.ENABLE = False\n",
        "# Provide path to prediction results for visualization.\n",
        "# This is a pickle file of [prediction_tensor, label_tensor]\n",
        "_C.TENSORBOARD.PREDICTIONS_PATH = \"\"\n",
        "# Path to directory for tensorboard logs.\n",
        "# Default to to cfg.OUTPUT_DIR/runs-{cfg.TRAIN.DATASET}.\n",
        "_C.TENSORBOARD.LOG_DIR = \"\"\n",
        "# Path to a json file providing class_name - id mapping\n",
        "# in the format {\"class_name1\": id1, \"class_name2\": id2, ...}.\n",
        "# This file must be provided to enable plotting confusion matrix\n",
        "# by a subset or parent categories.\n",
        "_C.TENSORBOARD.CLASS_NAMES_PATH = \"\"\n",
        "\n",
        "# Path to a json file for categories -> classes mapping\n",
        "# in the format {\"parent_class\": [\"child_class1\", \"child_class2\",...], ...}.\n",
        "_C.TENSORBOARD.CATEGORIES_PATH = \"\"\n",
        "\n",
        "# Config for confusion matrices visualization.\n",
        "_C.TENSORBOARD.CONFUSION_MATRIX = CfgNode()\n",
        "# Visualize confusion matrix.\n",
        "_C.TENSORBOARD.CONFUSION_MATRIX.ENABLE = False\n",
        "# Figure size of the confusion matrices plotted.\n",
        "_C.TENSORBOARD.CONFUSION_MATRIX.FIGSIZE = [8, 8]\n",
        "# Path to a subset of categories to visualize.\n",
        "# File contains class names separated by newline characters.\n",
        "_C.TENSORBOARD.CONFUSION_MATRIX.SUBSET_PATH = \"\"\n",
        "\n",
        "# Config for histogram visualization.\n",
        "_C.TENSORBOARD.HISTOGRAM = CfgNode()\n",
        "# Visualize histograms.\n",
        "_C.TENSORBOARD.HISTOGRAM.ENABLE = False\n",
        "# Path to a subset of classes to plot histograms.\n",
        "# Class names must be separated by newline characters.\n",
        "_C.TENSORBOARD.HISTOGRAM.SUBSET_PATH = \"\"\n",
        "# Visualize top-k most predicted classes on histograms for each\n",
        "# chosen true label.\n",
        "_C.TENSORBOARD.HISTOGRAM.TOPK = 10\n",
        "# Figure size of the histograms plotted.\n",
        "_C.TENSORBOARD.HISTOGRAM.FIGSIZE = [8, 8]\n",
        "\n",
        "# Config for layers' weights and activations visualization.\n",
        "# _C.TENSORBOARD.ENABLE must be True.\n",
        "_C.TENSORBOARD.MODEL_VIS = CfgNode()\n",
        "\n",
        "# If False, skip model visualization.\n",
        "_C.TENSORBOARD.MODEL_VIS.ENABLE = False\n",
        "\n",
        "# If False, skip visualizing model weights.\n",
        "_C.TENSORBOARD.MODEL_VIS.MODEL_WEIGHTS = False\n",
        "\n",
        "# If False, skip visualizing model activations.\n",
        "_C.TENSORBOARD.MODEL_VIS.ACTIVATIONS = False\n",
        "\n",
        "# If False, skip visualizing input videos.\n",
        "_C.TENSORBOARD.MODEL_VIS.INPUT_VIDEO = False\n",
        "\n",
        "\n",
        "# List of strings containing data about layer names and their indexing to\n",
        "# visualize weights and activations for. The indexing is meant for\n",
        "# choosing a subset of activations outputed by a layer for visualization.\n",
        "# If indexing is not specified, visualize all activations outputed by the layer.\n",
        "# For each string, layer name and indexing is separated by whitespaces.\n",
        "# e.g.: [layer1 1,2;1,2, layer2, layer3 150,151;3,4]; this means for each array `arr`\n",
        "# along the batch dimension in `layer1`, we take arr[[1, 2], [1, 2]]\n",
        "_C.TENSORBOARD.MODEL_VIS.LAYER_LIST = []\n",
        "# Top-k predictions to plot on videos\n",
        "_C.TENSORBOARD.MODEL_VIS.TOPK_PREDS = 1\n",
        "# Colormap to for text boxes and bounding boxes colors\n",
        "_C.TENSORBOARD.MODEL_VIS.COLORMAP = \"Pastel2\"\n",
        "# Config for visualization video inputs with Grad-CAM.\n",
        "# _C.TENSORBOARD.ENABLE must be True.\n",
        "_C.TENSORBOARD.MODEL_VIS.GRAD_CAM = CfgNode()\n",
        "# Whether to run visualization using Grad-CAM technique.\n",
        "_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.ENABLE = True\n",
        "# CNN layers to use for Grad-CAM. The number of layers must be equal to\n",
        "# number of pathway(s).\n",
        "_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.LAYER_LIST = []\n",
        "# If True, visualize Grad-CAM using true labels for each instances.\n",
        "# If False, use the highest predicted class.\n",
        "_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.USE_TRUE_LABEL = False\n",
        "# Colormap to for text boxes and bounding boxes colors\n",
        "_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.COLORMAP = \"viridis\"\n",
        "\n",
        "# Config for visualization for wrong prediction visualization.\n",
        "# _C.TENSORBOARD.ENABLE must be True.\n",
        "_C.TENSORBOARD.WRONG_PRED_VIS = CfgNode()\n",
        "_C.TENSORBOARD.WRONG_PRED_VIS.ENABLE = False\n",
        "# Folder tag to origanize model eval videos under.\n",
        "_C.TENSORBOARD.WRONG_PRED_VIS.TAG = \"Incorrectly classified videos.\"\n",
        "# Subset of labels to visualize. Only wrong predictions with true labels\n",
        "# within this subset is visualized.\n",
        "_C.TENSORBOARD.WRONG_PRED_VIS.SUBSET_PATH = \"\"\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------- #\n",
        "# Demo options\n",
        "# ---------------------------------------------------------------------------- #\n",
        "_C.DEMO = CfgNode()\n",
        "\n",
        "# Run model in DEMO mode.\n",
        "_C.DEMO.ENABLE = False\n",
        "\n",
        "# Path to a json file providing class_name - id mapping\n",
        "# in the format {\"class_name1\": id1, \"class_name2\": id2, ...}.\n",
        "_C.DEMO.LABEL_FILE_PATH = \"\"\n",
        "\n",
        "# Specify a camera device as input. This will be prioritized\n",
        "# over input video if set.\n",
        "# If -1, use input video instead.\n",
        "_C.DEMO.WEBCAM = -1\n",
        "\n",
        "# Path to input video for demo.\n",
        "_C.DEMO.INPUT_VIDEO = \"\"\n",
        "# Custom width for reading input video data.\n",
        "_C.DEMO.DISPLAY_WIDTH = 0\n",
        "# Custom height for reading input video data.\n",
        "_C.DEMO.DISPLAY_HEIGHT = 0\n",
        "# Path to Detectron2 object detection model configuration,\n",
        "# only used for detection tasks.\n",
        "_C.DEMO.DETECTRON2_CFG = \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\n",
        "# Path to Detectron2 object detection model pre-trained weights.\n",
        "_C.DEMO.DETECTRON2_WEIGHTS = \"detectron2://COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl\"\n",
        "# Threshold for choosing predicted bounding boxes by Detectron2.\n",
        "_C.DEMO.DETECTRON2_THRESH = 0.9\n",
        "# Number of overlapping frames between 2 consecutive clips.\n",
        "# Increase this number for more frequent action predictions.\n",
        "# The number of overlapping frames cannot be larger than\n",
        "# half of the sequence length `cfg.DATA.NUM_FRAMES * cfg.DATA.SAMPLING_RATE`\n",
        "_C.DEMO.BUFFER_SIZE = 0\n",
        "# If specified, the visualized outputs will be written this a video file of\n",
        "# this path. Otherwise, the visualized outputs will be displayed in a window.\n",
        "_C.DEMO.OUTPUT_FILE = \"\"\n",
        "# Frames per second rate for writing to output video file.\n",
        "# If not set (-1), use fps rate from input file.\n",
        "_C.DEMO.OUTPUT_FPS = -1\n",
        "# Input format from demo video reader (\"RGB\" or \"BGR\").\n",
        "_C.DEMO.INPUT_FORMAT = \"BGR\"\n",
        "# Draw visualization frames in [keyframe_idx - CLIP_VIS_SIZE, keyframe_idx + CLIP_VIS_SIZE] inclusively.\n",
        "_C.DEMO.CLIP_VIS_SIZE = 10\n",
        "# Number of processes to run video visualizer.\n",
        "_C.DEMO.NUM_VIS_INSTANCES = 2\n",
        "\n",
        "# Path to pre-computed predicted boxes\n",
        "_C.DEMO.PREDS_BOXES = \"\"\n",
        "# Whether to run in with multi-threaded video reader.\n",
        "_C.DEMO.THREAD_ENABLE = False\n",
        "# Take one clip for every `DEMO.NUM_CLIPS_SKIP` + 1 for prediction and visualization.\n",
        "# This is used for fast demo speed by reducing the prediction/visualiztion frequency.\n",
        "# If -1, take the most recent read clip for visualization. This mode is only supported\n",
        "# if `DEMO.THREAD_ENABLE` is set to True.\n",
        "_C.DEMO.NUM_CLIPS_SKIP = 0\n",
        "# Path to ground-truth boxes and labels (optional)\n",
        "_C.DEMO.GT_BOXES = \"\"\n",
        "# The starting second of the video w.r.t bounding boxes file.\n",
        "_C.DEMO.STARTING_SECOND = 900\n",
        "# Frames per second of the input video/folder of images.\n",
        "_C.DEMO.FPS = 30\n",
        "# Visualize with top-k predictions or predictions above certain threshold(s).\n",
        "# Option: {\"thres\", \"top-k\"}\n",
        "_C.DEMO.VIS_MODE = \"thres\"\n",
        "# Threshold for common class names.\n",
        "_C.DEMO.COMMON_CLASS_THRES = 0.7\n",
        "# Theshold for uncommon class names. This will not be\n",
        "# used if `_C.DEMO.COMMON_CLASS_NAMES` is empty.\n",
        "_C.DEMO.UNCOMMON_CLASS_THRES = 0.3\n",
        "# This is chosen based on distribution of examples in\n",
        "# each classes in AVA dataset.\n",
        "_C.DEMO.COMMON_CLASS_NAMES = [\n",
        "    \"watch (a person)\",\n",
        "    \"talk to (e.g., self, a person, a group)\",\n",
        "    \"listen to (a person)\",\n",
        "    \"touch (an object)\",\n",
        "    \"carry/hold (an object)\",\n",
        "    \"walk\",\n",
        "    \"sit\",\n",
        "    \"lie/sleep\",\n",
        "    \"bend/bow (at the waist)\",\n",
        "]\n",
        "# Slow-motion rate for the visualization. The visualized portions of the\n",
        "# video will be played `_C.DEMO.SLOWMO` times slower than usual speed.\n",
        "_C.DEMO.SLOWMO = 1\n",
        "\n",
        "def _assert_and_infer_cfg(cfg):\n",
        "    # BN assertions.\n",
        "    if cfg.BN.USE_PRECISE_STATS:\n",
        "        assert cfg.BN.NUM_BATCHES_PRECISE >= 0\n",
        "    # TRAIN assertions.\n",
        "    assert cfg.TRAIN.CHECKPOINT_TYPE in [\"pytorch\", \"caffe2\"]\n",
        "    assert cfg.TRAIN.BATCH_SIZE % cfg.NUM_GPUS == 0\n",
        "\n",
        "    # TEST assertions.\n",
        "    assert cfg.TEST.CHECKPOINT_TYPE in [\"pytorch\", \"caffe2\"]\n",
        "    assert cfg.TEST.BATCH_SIZE % cfg.NUM_GPUS == 0\n",
        "    assert cfg.TEST.NUM_SPATIAL_CROPS == 3\n",
        "\n",
        "    # RESNET assertions.\n",
        "    assert cfg.RESNET.NUM_GROUPS > 0\n",
        "    assert cfg.RESNET.WIDTH_PER_GROUP > 0\n",
        "    assert cfg.RESNET.WIDTH_PER_GROUP % cfg.RESNET.NUM_GROUPS == 0\n",
        "\n",
        "    # Execute LR scaling by num_shards.\n",
        "    if cfg.SOLVER.BASE_LR_SCALE_NUM_SHARDS:\n",
        "        cfg.SOLVER.BASE_LR *= cfg.NUM_SHARDS\n",
        "\n",
        "    # General assertions.\n",
        "    assert cfg.SHARD_ID < cfg.NUM_SHARDS\n",
        "    return cfg\n",
        "\n",
        "\n",
        "def get_cfg():\n",
        "    \"\"\"\n",
        "    Get a copy of the default config.\n",
        "    \"\"\"\n",
        "    return _assert_and_infer_cfg(_C.clone())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNe06OiGtdIS",
        "outputId": "dbddac19-41c1-46dc-8f7c-ebff524233ef"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/TimeSformer/timesformer/config/defaults.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell is for manually changing the test configuration to desired values (used for modifying the inference process)"
      ],
      "metadata": {
        "id": "YhYyDymK6OtL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def update_test_congif():\n",
        "%%writefile /content/TimeSformer/configs/Kinetics/TimeSformer_divST_8x32_224_TEST.yaml\n",
        "TRAIN:\n",
        "  ENABLE: False\n",
        "  DATASET: kinetics\n",
        "  BATCH_SIZE: 8\n",
        "  EVAL_PERIOD: 5\n",
        "  CHECKPOINT_PERIOD: 5\n",
        "  AUTO_RESUME: True\n",
        "DATA:\n",
        "  PATH_TO_DATA_DIR: /path/to/kinetics/\n",
        "  NUM_FRAMES: 32\n",
        "  SAMPLING_RATE: 32\n",
        "  TRAIN_JITTER_SCALES: [256, 320]\n",
        "  TRAIN_CROP_SIZE: 224\n",
        "  TEST_CROP_SIZE: 224\n",
        "  INPUT_CHANNEL_NUM: [3]\n",
        "TIMESFORMER:\n",
        "  ATTENTION_TYPE: 'divided_space_time'\n",
        "SOLVER:\n",
        "  BASE_LR: 0.005\n",
        "  LR_POLICY: steps_with_relative_lrs\n",
        "  STEPS: [0, 11, 14]\n",
        "  LRS: [1, 0.1, 0.01]\n",
        "  MAX_EPOCH: 15\n",
        "  MOMENTUM: 0.9\n",
        "  WEIGHT_DECAY: 1e-4\n",
        "  OPTIMIZING_METHOD: sgd\n",
        "MODEL:\n",
        "  MODEL_NAME: vit_base_patch16_224\n",
        "  NUM_CLASSES: 14\n",
        "  ARCH: vit\n",
        "  LOSS_FUNC: cross_entropy\n",
        "  DROPOUT_RATE: 0.5\n",
        "TEST:\n",
        "  ENABLE: True\n",
        "  DATASET: kinetics\n",
        "  BATCH_SIZE: 8\n",
        "  NUM_ENSEMBLE_VIEWS: 1\n",
        "  NUM_SPATIAL_CROPS: 3\n",
        "  CHECKPOINT_FILE_PATH: '/checkpoint/gedas/jobs/timesformer/kinetics_400/TimeSformer_divST_8x32_224/checkpoints/checkpoint_epoch_00025.pyth'\n",
        "DATA_LOADER:\n",
        "  NUM_WORKERS: 8\n",
        "  PIN_MEMORY: True\n",
        "NUM_GPUS: 1\n",
        "NUM_SHARDS: 1\n",
        "RNG_SEED: 0\n",
        "OUTPUT_DIR: ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmOlFjXptjSy",
        "outputId": "b153f1bb-5f59-45ec-fa61-154318c6da58"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/TimeSformer/configs/Kinetics/TimeSformer_divST_8x32_224_TEST.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell contains the main prediction script (runs the inference code on the user input videos and saves the results as a dictionary)"
      ],
      "metadata": {
        "id": "KOwfG1Cn6gvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict():\n",
        "\n",
        "  vid_name = \"\"\n",
        "\n",
        "  input_folder = \"/content/TimeSformer/input/\"\n",
        "\n",
        "  import os\n",
        "  import csv\n",
        "\n",
        "  # File processing\n",
        "  rows = []\n",
        "  video_names = []\n",
        "  for filename in os.listdir(input_folder):\n",
        "    if filename.endswith(\".mp4\"):\n",
        "      vid_path = input_folder + filename\n",
        "      rows.append([vid_path, 0])\n",
        "      video_names.append(filename)\n",
        "\n",
        "  dest = input_folder + \"test.csv\"\n",
        "  with open(dest, 'w', encoding='UTF8', newline='') as f:\n",
        "      writer = csv.writer(f)\n",
        "      writer.writerows(rows)\n",
        "\n",
        "  !python tools/run_net.py \\\n",
        "  --cfg configs/Kinetics/TimeSformer_divST_8x32_224_TEST.yaml \\\n",
        "  DATA.PATH_TO_DATA_DIR /content/TimeSformer/input/ \\\n",
        "  TEST.CHECKPOINT_FILE_PATH /content/TimeSformer/checkpoints/checkpoint_epoch_00015.pyth \\\n",
        "  TRAIN.ENABLE False \\\n",
        "  > /dev/null\n",
        "\n",
        "  print(\"Inference Complete!\")\n",
        "\n",
        "  # This section prints the prediction result\n",
        "  output = {}\n",
        "\n",
        "  import numpy as np\n",
        "  import pandas as pd\n",
        "  object = pd.read_pickle(r'/content/result.pkl')\n",
        "  tmp = []\n",
        "  results = [\"Not Crime\", \"Abuse\", \"Arrest\", \"Arson\", \"Assault\", \"Burglary\"\n",
        "              , \"Explosion\", \"Fighting\", \"Road Accidents\", \"Robbery\", \"Shooting\",\n",
        "            \"Shoplifting\", \"Stealing\", \"Vandalism\"]\n",
        "\n",
        "  num_vids = len(object[0])\n",
        "  for i in range(num_vids):\n",
        "\n",
        "    # Sort the scores\n",
        "    sorted_data = sorted(object[0][i], reverse=True)\n",
        "\n",
        "    # Obtain top3\n",
        "    high = sorted_data[0]\n",
        "    second = sorted_data[1]\n",
        "    third = sorted_data[2]\n",
        "\n",
        "\n",
        "    length = len(object[0][i])\n",
        "    index = 0\n",
        "    index2 = 0\n",
        "    index3 = 0\n",
        "    for j in range(length):\n",
        "      if object[0][i][j] == high:\n",
        "        index = j\n",
        "      elif object[0][i][j] == second:\n",
        "        index2 = j\n",
        "      elif object[0][i][j] == third:\n",
        "        index3 = j\n",
        "      \n",
        "\n",
        "    top3 = [results[index], results[index2], results[index3]]\n",
        "    output[video_names[i]] = top3\n",
        "    \n",
        "  \n",
        "  for filename in os.listdir(input_folder):\n",
        "    if filename.endswith(\".mp4\"):\n",
        "      vid_path = input_folder + filename\n",
        "      os.remove(vid_path)\n",
        "\n",
        "  return output\n",
        "\n",
        "# res = predict()"
      ],
      "metadata": {
        "id": "-hG4Ou4ktliP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell imports all the required packages for the UI"
      ],
      "metadata": {
        "id": "i4jUYCcu6HjA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import io\n",
        "import platform\n",
        "from urllib.request import urlopen\n",
        "\n",
        "import flasgger\n",
        "from flask_restful import Api\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from flask_restful import Resource, fields, marshal\n",
        "from flask import Flask, render_template_string, request, redirect, url_for\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "FUOpY_EWtwKF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell is for installing and authenticating ngrok (for the server port)"
      ],
      "metadata": {
        "id": "r4RiOduB6PGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /drive/ngrok-ssh\n",
        "%cd /drive/ngrok-ssh\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.tgz -O ngrok-stable-linux-amd64.tgz\n",
        "!tar -xf ngrok-stable-linux-amd64.tgz\n",
        "!cp /drive/ngrok-ssh/ngrok /ngrok\n",
        "!chmod +x /ngrok\n",
        "\n",
        "!/ngrok authtoken 27haTRGLxIObQwRQ5oNTh2oqYF8_4uowQBi9wHdhWEVoCfRNk\n",
        "%cd /content/TimeSformer\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoF64RBVtyB0",
        "outputId": "96214983-1dfb-4055-b754-34f3258f8c1d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/drive/ngrok-ssh\n",
            "--2022-05-05 08:07:01--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.tgz\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 54.161.241.46, 18.205.222.128, 54.237.133.81, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|54.161.241.46|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13770165 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.tgz’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.13M  12.8MB/s    in 1.0s    \n",
            "\n",
            "2022-05-05 08:07:03 (12.8 MB/s) - ‘ngrok-stable-linux-amd64.tgz’ saved [13770165/13770165]\n",
            "\n",
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n",
            "/content/TimeSformer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell contains the HTML code (Vue) for the UI"
      ],
      "metadata": {
        "id": "9uKjyGRy7UJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_template = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <script src=\"https://cdn.jsdelivr.net/npm/vue/dist/vue.js\"></script>\n",
        "    <script src=\"https://cdnjs.cloudflare.com/ajax/libs/axios/0.21.1/axios.min.js\"></script>\n",
        "    <meta charset=\"utf-8\"/>\n",
        "    <title>Classification Page</title>\n",
        "    <link href=\"https://fonts.googleapis.com/css?family=Roboto:300,400,500,700\" rel=\"stylesheet\">\n",
        "    <link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css\" rel=\"stylesheet\" integrity=\"sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3\" crossorigin=\"anonymous\">\n",
        "    <script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js\" integrity=\"sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p\" crossorigin=\"anonymous\"></script>\n",
        "    <style>\n",
        "      * {\n",
        "      box-sizing: border-box;\n",
        "      }\n",
        "      html, body {\n",
        "      min-height: 100vh;\n",
        "      padding: 0;\n",
        "      margin: 0;\n",
        "      font-family: Roboto, Arial, sans-serif;\n",
        "      font-size: 14px; \n",
        "      color: #0c20c9;\n",
        "      }\n",
        "      input, textarea { \n",
        "      outline: none;\n",
        "      }\n",
        "      body {\n",
        "      display: flex;\n",
        "      justify-content: center;\n",
        "      align-items: center;\n",
        "      padding: 20px;\n",
        "      background: #2f6fa3;\n",
        "      }\n",
        "      h1 {\n",
        "      margin-top: 0;\n",
        "      font-weight: 500;\n",
        "      }\n",
        "      form {\n",
        "      position: relative;\n",
        "      width: 80%;\n",
        "      border-radius: 30px;\n",
        "      background: #fff;\n",
        "      }\n",
        "      .form-left-decoration,\n",
        "      .form-right-decoration {\n",
        "      content: \"\";\n",
        "      position: absolute;\n",
        "      width: 50px;\n",
        "      height: 20px;\n",
        "      border-radius: 20px;\n",
        "      background: #2f6fa3;\n",
        "      }\n",
        "      .form-left-decoration {\n",
        "      bottom: 60px;\n",
        "      left: -30px;\n",
        "      }\n",
        "      .form-right-decoration {\n",
        "      top: 60px;\n",
        "      right: -30px;\n",
        "      }\n",
        "      .form-left-decoration:before,\n",
        "      .form-left-decoration:after,\n",
        "      .form-right-decoration:before,\n",
        "      .form-right-decoration:after {\n",
        "      content: \"\";\n",
        "      position: absolute;\n",
        "      width: 50px;\n",
        "      height: 20px;\n",
        "      border-radius: 30px;\n",
        "      background: #fff;\n",
        "      }\n",
        "      .form-left-decoration:before {\n",
        "      top: -20px;\n",
        "      }\n",
        "      .form-left-decoration:after {\n",
        "      top: 20px;\n",
        "      left: 10px;\n",
        "      }\n",
        "      .form-right-decoration:before {\n",
        "      top: -20px;\n",
        "      right: 0;\n",
        "      }\n",
        "      .form-right-decoration:after {\n",
        "      top: 20px;\n",
        "      right: 10px;\n",
        "      }\n",
        "      .circle {\n",
        "      position: absolute;\n",
        "      bottom: 80px;\n",
        "      left: -55px;\n",
        "      width: 20px;\n",
        "      height: 20px;\n",
        "      border-radius: 50%;\n",
        "      background: #fff;\n",
        "      }\n",
        "      .form-inner {\n",
        "      padding: 25px;\n",
        "      width: 100%;\n",
        "      text-align: center;\n",
        "      }\n",
        "      button {\n",
        "      width: 90%;\n",
        "      padding: 10px;\n",
        "      margin-top: 20px;\n",
        "      border-radius: 20px;\n",
        "      border: none;\n",
        "      border-bottom: 4px solid #1b6ec0;\n",
        "      background: #3986c5; \n",
        "      font-size: 16px;\n",
        "      font-weight: 400;\n",
        "      color: #fff;\n",
        "      }\n",
        "      button:hover {\n",
        "      background: #0b2474;\n",
        "      } \n",
        "      @media (min-width: 568px) {\n",
        "      form {\n",
        "      width: 100%;\n",
        "      }\n",
        "      .center {\n",
        "          margin: 0;\n",
        "          position: absolute;\n",
        "          top: 50%;\n",
        "          left: 50%;\n",
        "          -ms-transform: translate(-50%, -50%);\n",
        "          transform: translate(-50%, -50%);\n",
        "        }\n",
        "      .loading{\n",
        "        border: 16px solid #f3f3f3;\n",
        "        border-radius: 50%;\n",
        "        border-top: 16px solid #3498db;\n",
        "        width: 120px;\n",
        "        height: 120px;\n",
        "        -webkit-animation: spin 2s linear infinite; /* Safari */\n",
        "        animation: spin 2s linear infinite;\n",
        "        display: none;\n",
        "      }\n",
        "      }\n",
        "      /* Safari */\n",
        "      @-webkit-keyframes spin {\n",
        "        0% { -webkit-transform: rotate(0deg); }\n",
        "        100% { -webkit-transform: rotate(360deg); }\n",
        "      }\n",
        "\n",
        "      @keyframes spin {\n",
        "        0% { transform: rotate(0deg); }\n",
        "        100% { transform: rotate(360deg); }\n",
        "      }\n",
        "    </style>\n",
        "\n",
        "    <script>\n",
        "      function loading(){\n",
        "        document.getElementById(\"loading\").style.display = 'block';\n",
        "      }\n",
        "\n",
        "      function loadingStop(){\n",
        "        document.getElementById(\"loading\").style.display = 'none';\n",
        "      }\n",
        "    </script>\n",
        "\n",
        "  </head>\n",
        "  <body>\n",
        "\n",
        "     <!-- The APP UI -->\n",
        "    <div id=\"app\" style=\"display: flex; justify-content: center; align-items: center; padding: 20px; background: #2f6fa3;\">\n",
        "      <form id=\"imageForm\" enctype=\"multipart/form-data\" method=\"POST\" class=\"decor\">\n",
        "        <div class=\"form-left-decoration\"></div>\n",
        "        <div class=\"form-right-decoration\"></div>\n",
        "        <div class=\"circle\"></div>\n",
        "        \n",
        "        <div class=\"form-inner\">\n",
        "          \n",
        "          <h1 style=\"text-align: center;\">Select The Input Videos</h1>\n",
        "          <input id=\"imageFile\" name=\"file\" type=\"file\" accept=\"video/mp4,video/x-m4v,video/*\" multiple style=\"padding-left: 150px;\" />\n",
        "\n",
        "\n",
        "          <div class=\"spinner-border text-primary text-center\" role=\"status\" id=\"loading\" style=\"display: none; height: 2.5em; width: 2.5em; margin-left: 200px; margin-top: 10px\">\n",
        "            <span class=\"visually-hidden\">Loading...</span>\n",
        "          </div>\n",
        "\n",
        "          <button type=\"submit\" onclick=\"loading()\" > Run Classification </button>\n",
        "\n",
        "          <br></br>\n",
        "\n",
        "          <div id=\"res\">\n",
        "            \n",
        "          \n",
        "          </div>\n",
        "\n",
        "          <div id=\"vid\">\n",
        "\n",
        "          </div>\n",
        "\n",
        "        </div>\n",
        "  \n",
        "      </form>\n",
        "    </div>\n",
        "\n",
        "    <script>\n",
        "      <!-- The Vue application -->\n",
        "      var app = new Vue({\n",
        "          el: \"#app\",\n",
        "          data() {\n",
        "              return {\n",
        "                  image: null,\n",
        "                  prediction: null,\n",
        "              };\n",
        "          },\n",
        "      });\n",
        "\n",
        "      <!-- Calling the predict API when the form is submitted -->\n",
        "      document.getElementById(\"imageForm\").addEventListener(\"submit\", (e) => {\n",
        "          document.getElementById(\"res\").innerHTML = ''\n",
        "          axios\n",
        "              .post(\"/predict\", new FormData(document.getElementById(\"imageForm\")), {\n",
        "                  headers: {\n",
        "                      \"Content-Type\": \"multipart/form-data\",\n",
        "                  },\n",
        "              })\n",
        "              .then(\n",
        "                function(response){\n",
        "\n",
        "                  document.getElementById(\"res\").innerHTML =\n",
        "                  `<div id=\"container\">\n",
        "                  \n",
        "                  <div id=\"table\" style=\"background-color:white; width: 100%; margin: auto;\">\n",
        "                  <table class=\"table\">\n",
        "                    <thead>\n",
        "                      <tr>\n",
        "                        <th scope=\"col\" style=\"font-size: 1.35em;\">Video Name</th>\n",
        "                        <th scope=\"col\" style=\"font-size: 1.35em;\">Best</th>\n",
        "                        <th scope=\"col\" style=\"font-size: 1.35em;\">Second</th>\n",
        "                        <th scope=\"col\" style=\"font-size: 1.35em;\">Third</th>\n",
        "                      </tr>\n",
        "                    </thead>\n",
        "                    <tbody id=\"tabledData\">\n",
        "                    `\n",
        "\n",
        "                  \n",
        "                    let resList = response.data\n",
        "                    for (let video in resList){\n",
        "                      document.getElementById(\"tabledData\").innerHTML += ('<tr>' + '<td>' + video + '</td>' + '<td>' + resList[video][0] + '</td>' + '<td>' + resList[video][1] + '</td>' + '<td>' + resList[video][2] + '</td>' + '</tr>')\n",
        "                    }\n",
        "                    \n",
        "\n",
        "                    document.getElementById(\"res\").innerHTML +=\n",
        "                      `     </tbody>\n",
        "                          </table>\n",
        "                        </div><br><br>\n",
        "                      </div>\n",
        "                      `\n",
        "\n",
        "                  \n",
        "                  \n",
        "                    document.getElementById(\"loading\").style.display = 'none';\n",
        "                }\n",
        "              );\n",
        "\n",
        "          e.preventDefault();\n",
        "      });\n",
        "\n",
        "    </script>\n",
        "    \n",
        "  </body>\n",
        "</html>\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "LkBqZPv0yv4i"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the main cell starts the Flask app, serves the previously created Vue template to the server, and handles the user input and prediction output"
      ],
      "metadata": {
        "id": "uX1b7Tls7eNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "app = Flask(__name__)\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Serve the Vue template with the interactive UI\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "  return index_template\n",
        "\n",
        "# Classification API\n",
        "@app.route(\"/predict\", methods=['POST'])\n",
        "def predict_api():\n",
        "\n",
        "  \n",
        "  if request.files:\n",
        "    # links = []\n",
        "    files = request.files.getlist(\"file\")\n",
        "    for vid_file in files:\n",
        "      vid_file.save(os.path.join(\"/content/TimeSformer/input\", vid_file.filename))\n",
        "      # links.append(os.path.join(\"/content/TimeSformer/input\", vid_file.filename))\n",
        "\n",
        "    print(\"File upload complete\")\n",
        "    \n",
        "    # Get the results object\n",
        "    class_name = predict()\n",
        "    \n",
        "\n",
        "  else:\n",
        "    class_name = \"None\"\n",
        "\n",
        "  return class_name\n",
        "\n",
        "@app.route(\"/results\")\n",
        "def send_results():\n",
        "  res = request.args.get('res')\n",
        "  return render_template_string(res)\n",
        "\n",
        "# Run the app\n",
        "run_with_ngrok(app)\n",
        "\n",
        "# Uncomment below to run the app\n",
        "app.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbeI3Uibt6MQ",
        "outputId": "68e93561-cdbd-4eed-d59c-4528a23c7d02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__' (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " * Running on http://127.0.0.1:5000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Running on http://3940-34-90-75-6.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [05/May/2022 09:19:02] \"GET / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [05/May/2022 09:19:03] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File upload complete\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [05/May/2022 09:20:23] \"POST /predict HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference Complete!\n"
          ]
        }
      ]
    }
  ]
}